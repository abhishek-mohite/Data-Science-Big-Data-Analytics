1) Read the given datasets 'bank_marketing.csv'

Kindly use this attachment Bank Marketing Data Description
Load the necessary libraries 
 load the given dataset and prepare the dataset by following the steps given below:

•	Ensure the datatypes of the columns are appropriate
•	Drop the variable “Unnamed: 0”, missing values, duplicated values
•	Convert the categorical variables into dummy variables
•	Split the data into train (70%), test (30%) and set random state as 0

Build a KNN model using the train dataset (k=7) and. Using the model that has been built, answer the following question.

What is the value of accuracy of the model on the test dataset?

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder, OneHotEncoder

# Step 1: Load the dataset
df = pd.read_csv('bank_marketing.csv')

# Step 2: Ensure the datatypes of the columns are appropriate

# Assuming you have the data description for reference, you can convert data types as needed
# Example: df['age'] = df['age'].astype('int64')
# Make sure to adjust data types for other columns as per the description

# Step 3: Drop unnecessary columns, missing values, and duplicated values
df = df.drop(columns=['Unnamed: 0'])  # Drop the 'Unnamed: 0' column
df = df.dropna()  # Drop rows with missing values
df = df.drop_duplicates()  # Drop duplicated rows

# Step 4: Convert categorical variables into dummy variables

# Assuming you want to convert categorical columns to dummies
categorical_columns = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']

for column in categorical_columns:
    df = pd.concat([df, pd.get_dummies(df[column], prefix=column)], axis=1)
    df = df.drop(columns=[column])

# Step 5: Split the data into train (70%) and test (30%) with a random state of 0
X = df.drop(columns=['y'])
y = df['y']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

# Step 6: Build a KNN model using the train dataset (k=7)
knn_model = KNeighborsClassifier(n_neighbors=7)
knn_model.fit(X_train, y_train)

# Step 7: Make predictions on the test dataset
y_pred = knn_model.predict(X_test)

# Step 8: Calculate the accuracy of the model on the test dataset
accuracy = accuracy_score(y_test, y_pred)

print(f"Accuracy of the KNN model on the test dataset: {accuracy:.2f}")



2) Read the given datasets” bank_train.csv” for training the model
Read the given datasets” bank_test.csv”  for testing the model
Kindly use this attachment Bank Marketing Data Description
Drop the missing values for training and test data.
spilt the indpendent and dependent variables for test data and train data
Build a KNN model  for classification using the train dataset (k=11)
What is the accuracy of the model? 
How many samples are misclassified by the model?

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Load the training and testing datasets
train_data = pd.read_csv('bank_train.csv')
test_data = pd.read_csv('bank_test.csv')

# Drop missing values in both training and testing datasets
train_data = train_data.dropna()
test_data = test_data.dropna()

# Split the independent (features) and dependent (target) variables for both datasets
X_train = train_data.drop(columns=['y'])
y_train = train_data['y']
X_test = test_data.drop(columns=['y'])
y_test = test_data['y']

# Build a KNN model for classification using the train dataset (k=11)
k = 11
knn_model = KNeighborsClassifier(n_neighbors=k)
knn_model.fit(X_train, y_train)

# Predict the target values for the test dataset
y_pred = knn_model.predict(X_test)

# Calculate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)

# Count the number of misclassified samples
misclassified_samples = (y_test != y_pred).sum()

print(f"Accuracy of the KNN model: {accuracy:.2f}")
print(f"Number of misclassified samples: {misclassified_samples}")


3) Read the data Uber dataset
The dataset contains the following fields:
●	key - a unique identifier for each trip
●	fare_amount - the cost of each trip in usd
●	pickup_datetime - date and time when the meter was engaged
●	passenger_count - the number of passengers in the vehicle (driver entered value)
●	pickup_longitude - the longitude where the meter was engaged
●	pickup_latitude - the latitude where the meter was engaged
●	dropoff_longitude - the longitude where the meter was disengaged
●	dropoff_latitude - the latitude where the meter was disengaged
. Perform following tasks: 
1. Pre-process the dataset. 
2. Identify outliers. 
3. Implement linear regression model to  predict the price of the Uber ride from a given pickup point     to the agreed drop-off location 
5. Find out RMSE score 

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Step 1: Load and preprocess the dataset
uber_data = pd.read_csv('Uber_dataset.csv')

# You may want to handle any missing values, convert data types, and perform any data cleaning if necessary.

# Step 2: Identify and handle outliers (You can use a simple approach to detect outliers)
outliers = uber_data[(np.abs(uber_data['fare_amount'] - uber_data['fare_amount'].mean()) > 3 * uber_data['fare_amount'].std())]

# Remove outliers from the dataset
uber_data = uber_data[~uber_data.index.isin(outliers.index)]

# Step 3: Implement a linear regression model
X = uber_data[['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']]
y = uber_data['fare_amount']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Create and train a linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model.predict(X_test)

# Step 4: Calculate RMSE score
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f"Root Mean Square Error (RMSE): {rmse:.2f}")


4) Read the data Uber dataset
The dataset contains the following fields:
●	key - a unique identifier for each trip
●	fare_amount - the cost of each trip in usd
●	pickup_datetime - date and time when the meter was engaged
●	passenger_count - the number of passengers in the vehicle (driver entered value)
●	pickup_longitude - the longitude where the meter was engaged
●	pickup_latitude - the latitude where the meter was engaged
●	dropoff_longitude - the longitude where the meter was disengaged
●	dropoff_latitude - the latitude where the meter was disengaged
Perform following tasks: 
1. Pre-process the dataset. 
2. Identify outliers. 
3. Implement random forest  model to  predict the price of the Uber ride from a given pickup point     to the agreed drop-off location 
5. Find out RMSE score 

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

# Step 1: Load and preprocess the dataset
uber_data = pd.read_csv('Uber_dataset.csv')

# You may want to handle any missing values, convert data types, and perform data cleaning if necessary.

# Step 2: Identify and handle outliers (You can use a simple approach to detect outliers)
outliers = uber_data[(np.abs(uber_data['fare_amount'] - uber_data['fare_amount'].mean()) > 3 * uber_data['fare_amount'].std())]

# Remove outliers from the dataset
uber_data = uber_data[~uber_data.index.isin(outliers.index)]

# Step 3: Implement a Random Forest model
X = uber_data[['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count']]
y = uber_data['fare_amount']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Create and train a Random Forest model
model = RandomForestRegressor(n_estimators=100, random_state=0)
model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model.predict(X_test)

# Step 4: Calculate RMSE score
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f"Root Mean Square Error (RMSE): {rmse:.2f}")


5) Given a bank customer data (churn_modelling.csv) , build a neural network-based classifier that can determine whether they will leave or not in the next 6 months. 
Dataset Description: The case study is from an open-source dataset from Kaggle. 
The dataset contains 10,000 sample points with 14 distinct features such as 
CustomerId, CreditScore, Geography, Gender, Age, Tenure, Balance, etc. 
Link to the Kaggle project: 
Perform following steps: 
1. Read the dataset. 
2. Distinguish the feature and target set and divide the data set into training and test sets. 
3. Normalize the train and test data. 
4. Initialize and build the model. 
5 Print the accuracy score and confusion matrix 

import pandas as pd

# Load the dataset
data = pd.read_csv('churn_modelling.csv')

from sklearn.model_selection import train_test_split

# Define features (X) and target (y)
X = data.drop(columns=['Exited'])  # Exited is the target column
y = data['Exited']

# Split the data into training and test sets (e.g., 70% training, 30% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

from sklearn.preprocessing import StandardScaler

# Normalize features (standardization)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

from keras.models import Sequential
from keras.layers import Dense

# Initialize the model
model = Sequential()

# Add input layer and hidden layers
model.add(Dense(units=64, input_dim=X_train.shape[1], activation='relu'))
model.add(Dense(units=32, activation='relu'))

# Add output layer
model.add(Dense(units=1, activation='sigmoid'))

# Compile the model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)

from sklearn.metrics import accuracy_score, confusion_matrix

# Predict using the model
y_pred = model.predict(X_test)
y_pred = (y_pred > 0.5).astype(int)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

# Print confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(conf_matrix)


6) Given the medical data (diabetics.csv) having features like 
Pregnancies	Glucose	BloodPressure	SkinThickness	Insulin	BMI	Pedigree	Age

Implement K-Nearest Neighbors algorithm on diabetes.csv dataset to predict whether person will be diabetic or not?  
Compute confusion matrix,  accuracy, error rate, precision and recall on the given dataset. 


import pandas as pd

# Load the dataset
data = pd.read_csv('diabetes.csv')

# Define features (X) and target (y)
X = data[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'Pedigree', 'Age']]
y = data['Outcome']

from sklearn.model_selection import train_test_split

# Split the data into training and test sets (e.g., 70% training, 30% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

from sklearn.neighbors import KNeighborsClassifier

# Initialize the KNN model with a chosen value of k (e.g., k=3)
knn = KNeighborsClassifier(n_neighbors=3)

# Train the model on the training data
knn.fit(X_train, y_train)

from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score

# Make predictions on the test data
y_pred = knn.predict(X_test)

# Compute the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)

# Calculate error rate (misclassification rate)
error_rate = 1 - accuracy

# Calculate precision
precision = precision_score(y_test, y_pred)

# Calculate recall
recall = recall_score(y_test, y_pred)

print("Confusion Matrix:")
print(conf_matrix)
print(f"Accuracy: {accuracy:.2f}")
print(f"Error Rate: {error_rate:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")

